---
title: "Using Corpora in Social Science Research (UCSSR)"
subtitle: 'Basics of Sentiment Analysis'
author: "Andreas Blaette"
date: "25. Juli 2018"
output:
  ioslides_presentation: default
  beamer_presentation: default
  slidy_presentation:
    footer: Copyright (c) 2018, Andreas Blaette
editor_options: 
  chunk_output_type: console
---

```{r klippy, echo=FALSE, include=TRUE}
klippy::klippy()
```

## Fundamentals of sentiment analysis

Sentiment analyses are very popular. Text Mining blogs are showing the many possibilities to capture the variation of text evaluations with a numerical indicator and how to analyse and display changes over time.

Which movies are rated particularly good or particularly bad? This can be examined by using film reviews. What is the response of customers to a new product launched on the market? Comments in social media can be examined for this purpose. There is certainly a range of useful application scenarios for sentiment analyses, especially beyond science.

What are the benefits of sentiment analyses in scientific work? The central questions here are what one actually measures when one measures "sentiments". Is the validity of the measurement given, does one measure what one believes to measure? From the answers it can be derived when and how sentiment analyses can be used as a well-founded research instrument. The essential difference is as follows:

  * _Dictionary-based_ methods measure using lists with positive / negative vocabulary.
  * _Machine Learning_ based methods are developed from training data with known evaluations and make derivations for texts to be reassessed using an algorithm. 
  
In this manual we work with a, much simpler, dictionary-based method and a classic dictionary, "SentiWS".


---- 

## Required installations / packages

The following explanations use the `polmineR` package and the `GermaParl` corpus. The installation is explained in a separate set of slides. In addition, we use the following packages:

  * `zoo`: A package for working with time series data;
  * `magrittr`: Tools for chaining R commands in a "pipe" one after the other (see below);
  * `devtools`: developer tools, we use a command to download a single function;

The following code checks whether these packages are installed and installs them if necessary.

```{r install_required_packages, eval = TRUE}
required_packages <- c("zoo", "magrittr", "devtools")
for (pkg in required_packages){
  if (pkg %in% rownames(installed.packages()) == FALSE) install.packages(pkg)
}
```

Please note that the functionality for the following workflow is only available with polmineR version `r as.package_version("0.7.9.9005")`. If required, install the current development version of polmineR.

```{r update_polmineR, eval = TRUE}
if (packageVersion("polmineR") < as.package_version("0.7.9.9005"))
  devtools::install_github("PolMine/polmineR", ref = "dev")
```


----


## Let's go

The required packages are now loaded.

```{r load_libraries, eval = TRUE}
library(zoo, quietly = TRUE, warn.conflicts = FALSE)
library(devtools)
library(magrittr)
library(data.table)
```

We also load polmineR and activate the GermaParl corpus, which is available through the GermaParl package.

```{r load_polmineR, eval = TRUE}
library(polmineR)
use("GermaParl")
```

----

## Source in function 'get_sentiws' from a GitHub gist

In the following exemplary analysis we use the sentiment dictionary of the Leipzig Vocabulary Project, in short "SentiWS". You can find an explanation [here](http://wortschatz.uni-leipzig.de/de/download). The data is available under a CC-BY-SA-NC license and can be used in a scientific context without licensing restrictions. It goes without saying that the data must be quoted correctly.

SentWS can be downloaded as a zip file. To make things even easier, the GitHub presence of the PolMine project includes a function that does the download and automatically generates a tabular data structure as we will need it.

```{r get_senti_ws, eval = TRUE}
gist_url <- "https://gist.githubusercontent.com/PolMine/70eeb095328070c18bd00ee087272adf/raw/c2eee2f48b11e6d893c19089b444f25b452d2adb/sentiws.R"
devtools::source_url(gist_url) # after execution, the function is available
SentiWS <- get_sentiws()
```

Now the object `SentiWS` represents the available sentiment dictionary.

----

## SentiWS: First look at the data

The SentiWS object is a `data.table` object. We use this instead of a classic `data.frame`, because it will later facilitate and accelerate the matching of the data (words in the corpus and words in the dictionary). To understand what we are working with, we take a quick look at the data.

```{r inspect_senti_ws, eval = TRUE}
head(SentiWS, 5)
```

In the last column ("weight") you can see that words are assigned a weighting in the dictionary. This can be positive (for "positive" vocabulary) or negative (for "negative" words). You can also see that in addition to the word column, lemmata and a Part-of-Speech classification are also listed. The latter makes a clear assignment possible. The column "base" with a logical value (TRUE/FALSE) results more technically from the conversion of the table downloaded from the Leipzig Vocabulary Project into an extensive form: There is a lemma in each row of the raw table. Accordingly, we can check how many positive or negative words are in the table as a basic form.

```{r, eval = TRUE}
vocab <- c(positive = nrow(SentiWS[base == TRUE][weight > 0]), negative = nrow(SentiWS[base == TRUE][weight < 0]))
vocab
```

----

# Positive / negative vocabulary in word environments

We now examine the word environment of an interesting term. Because it is so simple, relevant and bold, we ask how the positive/negative connotations of Islam have developed over time.

A preliminary question is how large the left and right word contexts to be examined should be. In linguistic studies, a context of five words left and right is common. More may be needed for political assignments of meaning. We start from 10 words and set this for our R-session as follows.

```{r}
options("polmineR.left" = 10L)
options("polmineR.right" = 10L)
```

Via a "pipe" we now generate a `data.frame` ("df") with the counts of the SentiWS vocabulary in the word environment of "Islam". The pipe makes it possible to carry out the steps one after the other without saving intermediate results.

```{r}
df <- context("GERMAPARL", query = "Islam", p_attribute = c("word", "pos"), verbose = FALSE) %>%
  partition_bundle(node = FALSE) %>%
  set_names(s_attributes(., s_attribute = "date")) %>%
  weigh(with = SentiWS) %>%
  summary()
```

----

## The tabular data of the sentiment analysis

The df-data.frame lists the statistics of the word surroundings of each occurrence of "Islam" in the corpus. To keep things simple, we do not initially work with the weightings, but only with the positive or negative words. We simplify the table accordingly and look at it.

```{r}
df <- df[, c("name", "size", "positive_n", "negative_n")] 
head(df, n = 15)
```



----

# Aggregation

As name of a word context we used the date of the occurrence of our search term above. This makes it possible to aggregate upwards on the basis of the date for the year.


```{r}
df[["year"]] <- as.Date(df[["name"]]) %>% format("%Y-01-01")
df_year <- aggregate(df[,c("size", "positive_n", "negative_n")], list(df[["year"]]), sum)
colnames(df_year)[1] <- "year"
```

However, it does not make sense to work with the absolute frequencies. Therefore, we insert columns that indicate the proportion of negative or positive vocabulary.

```{r}
df_year$negative_share <- df_year$negative_n / df_year$size
df_year$positive_share <- df_year$positive_n / df_year$size
```

We convert this into a time series object in the actual sense.

```{r}
Z <- zoo(
  x = df_year[, c("positive_share", "negative_share")],
  order.by = as.Date(df_year[,"year"])
)
```


----

## Visualisation

```{r}
plot(
  Z, ylab = "polarity", xlab = "year", main = "Word context of 'Islam': Share of positive/negative vocabulary",
  cex = 0.8, cex.main = 0.8
)
```

----

## How good are the results?

But what is actually behind the numerical values of the determined sentiment scores? To investigate this, we use the possibility of polmineR to reduce a KWIC output according to a positive-list (vector with required words), to color-code words and to show further information via tooltips (here: word weights). So: Move your mouse over the highlighted words!

```{r, eval = TRUE}
words_positive <- SentiWS[weight > 0][["word"]]
words_negative <- SentiWS[weight < 0][["word"]]
```

```{r, eval = TRUE}
kwic("GERMAPARL", query = "Islam", positivelist = c(words_positive, words_negative)) %>%
  highlight(lightgreen = words_positive, orange = words_negative) %>%
  tooltips(setNames(SentiWS[["word"]], SentiWS[["weight"]])) %>%
  as("htmlwidget") -> Y
```

The result stored in object Y (a 'htmlwidget') is shown on a seperate slide.

----

```{r, eval = TRUE, echo = FALSE}
Y
```

----

## Discussion

  * How do you interpret the results of the time series analysis?
  * How valid are the results?
  * Is the transition to working with word weightings useful or necessary?